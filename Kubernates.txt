kubectl version --short
Skooner: K8s dashboard
Container orchestration automates the deployment,management,scaling and networkng of containers across the cluster. it focus on managing the lifecycle of containers. 
K8s  -> open-source platform for managing containarized workload and services. 

Verticaal scaling: To scale more, add one or more ram,cpu,memory to the one existing machine.
Horizontal scaling: To scale more, add more machines to the existing group of distributed systems.  

K8s objects:
compute: pod,node,namespace,deployment,Replicaset,statefulset,Deomonset
networking: service,ingress
storage: volume,persistentVolume, StorageClass, PersistentVolumeClaim

K8s Architechture overview:1.24

Control plane           	kubectl                              Nodes(30000-32767)                   
-----------------------------------↓-----------        --------------------------------------------------------
|________________           _______↓_________ |       |------->Kube Proxy Server|------>Kube Proxy Server    |
|Kube Controller|---------> |Kube API Server|<------->|                         |                            |
|____mgr_10252__|           |___6443________| |       |------->Kubelet 10250    |------>Kubectl              |
|                           ↑     |           |       |          ↓              |         ↓                  |
| Kube Scheduler (10251)----|     |           |       |     Container Runtime   |    Container Runtime       |                           
|                           ↑     ↓           |       |     (docker/containerd) |     (docker/containerd)    |
| Cloud Controller mgr------↑ETCDCTL(2379-80) |       |                         |                            |
|_____________________________________________|       |_________________________|____________________________|

kubectl: command line utility used to interact with k8s cluster. communicates with API server throuh REST API calls for CRUD workloads in kubernetes. 

Control plane: 
is collection of multi components responsible for managing/controling K8s cluster globally. 
Individual control plane components can run on any machine in the cluster. But usually run on dedicated controller machines.

Kube-API-Server --> serves K8s API. primary interface to control plane and cluster itself. When interacting with K8s cluster we do so using K8s API.
ETCD --> ETCD is key-value db, stores the state of K8s cluster. X store app or db data
Kube-Scheduler --> handles scheduling of pods, irrespective of whether pods are running or stopped. 
Kube-Controller-Manager --> brain behind orchestration. It does controller utilities like detecting failed pods and restarting them.(manage desired and current state)
Cloud controller manager --> provides an interface between K8s and cloud platform. It is only used when using cloud based resources with K8s. 


K8s Nodes: 
are machines where containers managed by the cluster run. cluster can have any number of  nodes. 
Various node components manage containers on machine & communicate with control plane.

Kubelet --> k8s agent runs on each node. It communicate with control plane(API server) about state of node and ensures that containers(through CRE i.e containerd or docker) run on its node as instructed by control plane. 
Kubelets also handles the process of reporting container status and other data about container back to API server. 
Container Runtime --> is not build into k8s. Its a separate peice of software that is responsible to run container on each machine. K8s supports docker/containerd etc
Kube-Proxy --> is network proxy. It runs on each node and provides networking between containers and services(single entrypoint which manages backend nodes) in the cluster by maintaining iptable.  

CNI: container networking interface maintains the routing rules for container communication inside cluster. 
CRI: container runtime interface acts as bridge between kubelets and container runtime. 
coreDNS: DNS service which resolves names to IP.
service: single entrypoint which manages backend nodes. pod to pod communication is very difficult without service as containers are not permanent. name and IP gets change if new container gets created.  so container can talk to service (virtual object. its not container rather configuraton) which manages the backend nodes(pods). 


-----Journey of kubectl request------

kubectl 	   kubectl				 kubectl					kube API server
 | 		  Client validation	--------------> form the HTTP request	-------------------->  server side validation		----------------> ETCD	
 ---------------> 1. check YAML syntax			1. forms API calls				1. performs authentication      --------------> controller 
 kubectl run	  2. misconfiguration			2. authentication and cert validation		 and authorization		 -------------> Scheduler
 kubectl apply	  3. non-supported resources		for user validation				2. verifies the API		-------------> kubelet +
		  4. fail fast mechanism to avoid	3. reads kubeconfig file									container
 		loading API server			4. submit pod spec to API server


1. kubectl forms API request and send it to the API server.
2. APi server authenticates and authorise
3. API server writes pod object to etcd data store. once write is successful, aknowladgement is sent to API server and to the client.
4. now pod is in pending state
5. Scheduler sees that new pod object is created but not bound to any node
6. Scheduler assigns node to the pod and udpates API server.
7. API server updates this to the etcd data store. 
8. kubelets keep querying the API server for any new work loads. it sees that new pod is assigned to it. 
9. kubelets instructs the container runtime like docker to create container and update the container state back to API server.
10. APi server update the pod state as running in etcd data store. 

kubectl	 <---------> API server	<--->	etcd			scheduler		kubelet		docker
			<-------------------------------------->
			<------------------------------------------------------------->
											 <-------------->	

pod : smallest deployable unit that holds and runs the container(one or more).
 
Kubernates cluster:
kubeadm is a tool that will simplify the process of setting of K8s cluster. 
install containerd/docker --> install K8s packages (kubeadm,kubelet,kubectl) --> initialize cluster with <sudo kubeadm init --pod-network-cidr range --kubernates-version 1.24.0
--> setup .kube config command from output for cluster communication --> execute join command on nodes from output --> deploy calico network plugin 

kubectl get namespaces   ---list namespaces
kubectl get pods --namespace or -n my-namepsace  --list pods from my-namespace if namespace name not given cmd will fetch pods from default namespace
kubectl get pods --all-namespaces
kubectl get pods -o wide --> get detailed info with PODS IP address and node name
kubectl get nodes --> list cluster nodes


High Availability in K8s:
K8s facilitates high availability apps but you can also design the cluster itself to be highly available. To do this you need multiple control plane nodes.

Control plane node1 ....control plane node2
kube-api-server		 Kube-api-server									
             ↑            ↑
              Load balancer
                 ↑
Worker node1     ↑
kubelets    -----↑

1. Stacked ETCD   ---> ETCD runs on same control plane node
2. External ETCD  ---> ETCD runs on separate machine but not on control plane node

K8s Management tool: 
There are variety of K8s management tools available. These tools interface with Kubernates to provide additonal functionality
Kubectl --> Official CLI for K8s. it is the main method you will work with K8s.
kubeadm --> is a tool that will simplify the process of setting of K8s cluster. 
Minikube --> allows you to automatically set up a local single-node k8s cluster. Its great for k8s dev purposes. 
Helm --> provides templating and package management for K8s objects. You can use it to manage your own templates (known as charts). you can download and use shared templates.
Kompose --> helps you translate docker compose files into K8s objects. If you use docker compose for some of your workflow then can move apps to K8s using Kompose.
Kustomize --> configuration management tool for managing K8s objects configurations.It allows you to share and re-use template configurations for K8s apps.


kubeconfig: file used by kubectl to retrieve the required configuration of kubernetes cluster or to communicate with API server of kubernetes cluster. 
by default, kubectl looks in below locations
1. $HOME/.kube dir
2. export KUBECONFIG=$KUBECONFIG:$HOME/.kube/config  			--> env variable
3. kubectl config --kubeconfig=config-demo view --minify		--> command line flag

kubeconfig file: user -->context,namespace --> cluster			--> context does the mapping of user to cluster 

context: refers to set of access parameters(cluster server,namespace,user) for kubernetes cluster. used when working with >1 clusters and have diff configurations.
kubectl config get-contexts/current-context/use-context <context-name> : Display available contexts/current context/Switch to a different context

---------------------------Pods-------------------------

Pod: smallest deployable unit that holds and runs the container(one or more). managed by controler manager. each pod has name and IP. containers within same pod shares same n/w,volume and namespace. pod spec is the YAML file that k8s use to describe about containers inside pod. 
pod lifecycle
1. pending: containers not created yet. in the state of downloading container image over n/w
2. running: pod is scheduled on node and atleast one container running state.
3. succeeded: pod exited with status 0 and will not restart
4. Failed: all containers in pod exited but atleast one with non-zero status
5. CrashLoopBackOff: container fail to start and tried again and again
6. Unknown: for some reason state of pod could not be obtained. 

container lifecycle:(waiting,running,terminated) 
pod restart policy:
  1. Always  --> default restart policy in K8s. containers always be restarted if they stop, or completed successfully
  2. On-Failure --> Will restart container only if container process exits with error code or containr detect to be unhealthy by liveness probe. X after successfully completed. 
  3. Never  --> never be restarted. even if container exits or liveness probe fails. 

multi-container pod design patterns:
It is not adviced to burden the main application with additinal responsibilties like logging,monitoring,conneting to dB etc.keeping the image as small as possible to reduce attack surface. 
1. Init Containers:Init containers are exactly like other containers except they always run to completion. init containers must complete successfully before next one starts. if pods init container fails K8s repeatadly restarts the pod untill init container succeeds. if restart policy set to Never then K8s treat overall pod as failed. you can use init containers to do variety of startup tasks. They are often useful in keeping main containers lighter and more secure by offloading startup tasks to separate container. (works like bootstrap script)

apiVersion: v1
kind: Pod
metadata:
  name: init-container-pod
spec:
  initContainers:
  - name: init
    image: busybox:latest
    command: ["/bin/sh", "-c"]
    args: ["i=10; while [ $i -ge 0 ]; do echo $i; sleep 0.5; i=$(expr $i - 1); done"]
  containers:
  - name: main-container
    image: busybox:latest
    command: ["/bin/sh", "-c"]
    args: ["echo 'Hello from main container'"]



2. Sidecar: sidecar pattern uses the helper container to enhance or extend the functionality of main container. it does additional functionalities like logging,monitoring,security etc. failures in sidecar will not impact main container.
apiVersion: v1
kind: Pod
metadata:
  name: sidecar-pod
spec:
  volumes:
  - name: temp
    emptyDir: {}

  containers:
  - name: side-car
    image: busybox:latest
    command: ["/bin/sh"]
    args: ["-c", "while true; do echo 'This is from $(hostname)' >> /var/log/index.html; sleep 1; done"]
    volumeMounts:
    - name: temp
      mountPath: /var/log

  - name: application
    image: nginx
    ports:
    - containerPort: 80
    volumeMounts: 
    - name: temp
      mountPath: /usr/share/nginx/html

3. Ambassador: proxy pattern. acts as gateway or mediator for n/w communication. often handling tasks such as routing,load balancing etc. handles requests/responses from/ to remote services or external services. 

4. Adaptor: serves as translator between diff components. plays crutial role in enabling interoperability between diverse tech or services. (Log format changer)

image pull policy:
determine if the container image should be pulled from repo prior starting container. 
value		description
always		always pull	if tag is latest
ifNotPresent	if not exist	(default)
never		never pull

ImagePullBackOff: status condition occur while pulling image (Authen issue,n/w issue, image not availble,registry rate limit,registry not available etc)
		To avoid above issue due to auth we can use imagepullsecrets at container level. 

pause container: used to share the n/w namespace among container in pod for internal container communication plus ensure that IP address remains constant in container restart. 

imparative/declarative commands: 
imparative: command line argument
	kubectl run web --image=nginx:latest	--dry-run=client	--> shows what API object is being created at client side
	kubectl get po -o wide/json/yaml				--> shows deployed pods
	kubectl run web --image=nginx --dry-run=client -o yaml		--> gives you manifest file
	kubectl run web --image=nginx --label="app=hazelcast,env=prod"	--> label is key value pair
	kubectl logs -f nginx						--> show pod logs. append -c containername -l key=value(label)	--tail=20 --since=1h
	kubectl delete po nginx	--force --grace-period=0		--> want to force kill. grace period is 30sec default
	kubectl edit po nginx						--> can update pod manifest file
	kubectl exec -it nginx -- command				--> exec command inside pod. if pod has >1 container then use -c containername. -- 										differentiates kubectl commands with actual sh command. 
	kunectl run curl --image=curlimages/curl -i --rm --restart=never -- curl IP		--> use --rm for temporary pods. 

	kubectl logs/describe (events)/events -n namespace_name		--> used for debugging

decalarative: describing k8s objects in yaml file
kubectl create -f podfile.yml				--> create prod
kubectl apply  -f podfile.yml				--> update pod	

Manifest/Spec file:   describe k8s objects in yaml file
Manifest file has 4 mandatory fields:
	1. apiVersion		--> kubectl api-resources 
	2. kind			--> type of resource(deployment,pod,service)
	3. metadata	
	4. spec			--> includes replicas,selector,template(spec--> containers) for pod etc
            default-container: <container name> 				--> if pod has multiple containers

-----man pages--------------
# list all K8s API supported objects
kubectl api-resources
kubectl explain pod

Note: command(ENTRYPOINT) and CMD(arg) in kubenertes. use --command -- <arg> 
ex: --command ping -- 8.8.8.8

Ephemeral containers: additional container(with debug utilities like bash/sh/curl etc) in pod for debug purpose.  Not restarted when exit or pod removed or restarted.
	|________> Application container(distroless image)

kubectl debug -it ephemeral-demo<pod name> --image=busybox:1.2 --target=ephemeral-demo		--> create debug container in ephemeral-demo pod

----Labels,annotations and Selectors:
Labels are key-value pairs attached to the K8s objects like pod. run = <object name> is deafult label
kubect get po --show-labels
kubectl label po <podname> env=prod tier=frontend		--> add label to running containers. --overrite if want change existing one. 
kubectl label po <podname> env-					--> use key- to remove label

labels allows you to identify,select,operate on k8s objects. where as annotations are non-identifying metadata. Annotations can hold any kind of info which can be useful and provide context to DevOps teams. 

kubectl annotate po <podname> key=value

selectors: used to filter K8s objects based on labels. 
1. equality based selectors (=, !=)		--> used by Replication Controllers, services
kubectl get po --selector env=prod
2. set-based selectors	(in,notin,exists())	--> filter keys acccording to set of values. used by ReplicaSets, Deployments,DaemonSets. 
kubectl get po --selector env in (prod,qa)	


--------------------Replica Sets and Replica Controllers-------------(Reliability,Load balancing and scaling)
ReplicationController:  is deprecated and has been replaced by ReplicaSets. they use equality based selectors. 
ReplicaSet:  is a lower-level controller that ensures a specified number of replica Pods are running based on desired and current state. use set based seletors. 

kubectl get rc/rs
kubectl delete -f replicaCntroller.yml
kubectl scale rc/rs nginx-rc/rs --replicas=10

Note: selector must be of same name as defined in pod metadata. otherwise throws error. 
ReplicaController.yml
apiVersion:v1
kind:ReplicaController / ReplicaSet
metadata:
  name: nginx-rc
spec:
  replica:3
  selector:
   app: nginx
  template:
    metadata:
     labels:			matchLabels:			--> here it will be matchLabels(AND operated) which shows set based selector for ReplicaSet. 
      app: nginx 			app: nginx	           we can use matchExpressions:(OR operated) for multiple selectors
    spec								       - key: app
      container:								 operator: In
      - name: nginx								 values: [nginx,prod]
        image: nginx
        port:
        - containerPort: 80

Note: if we manually create pod with same label as above then ReplicaController/Set terminate manually created pod by thinking Desired abd current pods number is same.  


-----------------Deployments and DeploymentController----------
Deployment:  is a higher-level controller that manages(internally creates) ReplicaSets and provides additional features like rolling updates,rollback,pause and resume update etc for deploying and updating applications.
Deployment is recommended way to deploy pod(deploymentname+rs-id+hash) or replicaSet. keep track of various deployed versions of apps for rollback. (not the case with ReplicaSet)

kubectl create deploy nginx --replica=5 --image=nginx:latest --port=80
kubectl get deploy
kubectl delete deployment nginx
kubectl rollout status deployment nginx
kubectl rollout history deplo nginx
kubectl set image deploy nginx nginx:1.4	--> change image of container.
kubectl scale deplo nginx --replica=10  

rollout: its a process of gradually deploying or updating your application containers. 
----deployment strategies:
1. recreate: old version is terminated and new version is rolled out. easy but expects downtime
2. Rolling/Ramped update: default strategy. new version is slowly rolled out by replacing old version. default 25% for both
	1. maxSurge: number of pods created on top of existing ones. 
	2. maxUnvailable: number of pods down at a time. 
Note: if maxSurge is 0 then maxUnvailable can't be 0
3. Blue-Green: initially user traffic goes to Blue(current version) deployment, when new version deployment ready, route traffic to green env.
having two env and switching between them for updates. 
4. Canary: testing new version with small group before rollout. 
5. A/B testing: comparing two versions by exposing diff user to each version. 


---------------Namespace----------------------
K8s Namespaces: K8s objects such as pods and containers, live in namespaces. namespaces provide a way to partition cluster resources between multiple users, teams, or applications. You can use namespaces to create isolated environments. 
4 default namespaces:
1. default : as name says its default namespace when we create any k8s object
2. kube-system - user dont have access to it. k8s object such as kubectl, api-server are created under kube-system namespace.
3. kube-public - publically accessible k8s objects are created user kube-public like Congfigmap, secrete etc
4. kube-node-lease - contains hear-beats of nodes in k8s cluster + each node has associated least object in namespace + determine availability of nodes

every resource gets unique name which is combinatin of (resource name + namespace) called FQDN. fully qualified domain name. 

#create namespace: my-namespace.yml
apiVersion: v1
kind: namespace
metadata:
  name: my-app-namespace

#define resource quota: resource-quota.yml
apiVersion: v1
kind: ResourceQuota
metadata:
 name: my-app-quota
 namespace: my-app-namespace			--> bad practice to hard code namespace rather pass -n <namespace name> while deployment
spec:
 hard:
   pods:"number"
   requests.memory:					Mi --> Mebibytes (more than megabyte 10^6 its decimal unit sotrage but computer understand in binary unit which
   requests.cpu:					is 2^20 little more han megabyte					
   limit.cpu:						m --> millicore
   limit.memory:


kubectl get ns							--> list namespaces
kubetl delete ns dev						--> very careful while deleting ns. we can assign roles to limit actions 
kubectl deploy -f specfile.yml -n <namespace name>		--> create deployment in perticular namespace

curl pod-ip.ns.pod.cluster.local				--> to communicate with pod in diff ns using pod-dns


---------------------Services--------------------------
service: single entrypoint which manages backend nodes. pod to pod communication is very difficult without service as containers are not permanent. name and IP gets change if new container gets created. so container can talk to service (virtual object. its not container rather configuraton) which manages the backend nodes(pods).

IP's are stable and balance the load. service identifies its member pods with selector so pod must have labels specified in selector of service. 
types of service:
1. ClusterIP: default. assigns stable internal IP to set of pods enabling communication within cluster. 
5. ExternalName

apiVersion:
kind: Service
metadata:
spec:
  selector:
  ports:
  - protocol: 
    port: incoming port to service
    targetPort: outgoing port should be same as container port
  type: ClusterIP						--> if not specified then default is ClusterIP


kubectl expose deploy web --port=80 --target-port=80 --type=ClusterIP    		--> clusterIp is default so no need to mention
kubectl get ep web									--> show ist of endpoinds managed by service

Note: Each pods use the information in /etc/resolv.conf to locate the DNS server, and CoreDNS(running in kube-system ns) is the DNS server responsible for resolving service names within the Kubernetes cluster.  kubeproxy maintains the IP table and share those with service. 

service FQDN: servicename.ns.svc(K8s obj).cluster.local
pods exposed by service gets dns like pod-ip.servicename.ns.svc.cluster.local

port-forwading: used for temp creating connection with pod for debug purpose. pods are not accessible directly from outside world without service expose. 
kubectl port-forward po/podname localport:podport
kubectl port-forward po/web 8080:80				--> forward data coming on local port 8080 to pod listening on port 80 

2. NodePort: exposing service on static port n each node, allowing external access to service. 
port must be in the raneg of 30000-32767. its recommended to et K8s auto assign the port. NodePort uses the labels and selectors as ClusterIP

spec:
  selector:
  ports:
  - protocol: 
    port: incoming port to service
    targetPort: outgoing port should be same as container port
    nodePort: let K8s auto assign
  type: NodePort

drawback: exposing service through nodeport uses multiple IPs and corresponding node port makes it difficult for end user to access apps makes it user unfriendly.
solution: Deploy HAProxy outside the cluster and configure it to balance traffic to the nodes using their NodePort.


3. LoadBalancer: is a way to expose services to the external world. When you create a service with type LoadBalancer, the cloud provider (if you're using a cloud-managed Kubernetes service) automatically provisions a load balancer and assigns it a public IP address. it internally creates nodeport. Load balancer accepted request on its static IP and forward request to nodeport on nodeport IP and port number.  

drawback: It can incur additional costs, especially in cloud environments where public IP addresses and load balancers are billable resources.
solution: Ingress controller with LB -> allow you to expose multiple services through a single load balancer and public IP address. 
load balancer sharing -> share a single load balancer among multiple services, reducing the number of provisioned IP addresses.


4. Headless Service(no IP allocated): (very imp for statefulset apps) It is used when you don't need a single, stable IP address for the service, and you want to directly reach individual pods backing the service.The service creates DNS records for each pod backing the service, allowing you to discover individual pod IPs using DNS. commonely used in database clusters, statefulsets etc

apiVersion:v1
kind: Service
metadata:
 name: my-headless-service
spec:
  ClusterIP: None
  ports:
  - protocol: tcp
    port: 80

Note: If we do nslookup on healess service FQDN then it resolves to backend pods Ip address.
nslookup my-headless-service.default.svc.cluster.local			--> gives backend pods IP's. each pod gets unique identity like podname.headless-servicename


External IP: The externalIPs feature is not primarily designed for managing external IP addresses of services, especially for external databases or external applications hosted outside the K8s cluster. Instead, it's used to assign specific external IP addresses to services within the Kubernetes cluster itself.
spec:
  selector:
  ports:
  - protocol: 
    port: incoming port to service
    targetPort: outgoing port should be same as container port
    nodePort: let K8s auto assign
  type: NodePort
  externalIPs:
  - 203.0.103.1


Services without pods: selectors from service used to connect to endpoints. but service without selector does not create endpoints so we manually create endpoints (like aws rds Ip:port)
The Endpoints resource in K8s is used to associate a service with a set of network addresses. 

apiVersion: v1
kind: Service
metadata:
 name: headless-service
spec:
 ClusterIP: None
---
apiVersion: v1
kind: Endpoint
metadata:
 name: my-headless-endpoint
spec:
 - addresses:
    - 201.0.102.1						--> it can be external apps IP address which can be connected from K8s cluster. 
   ports:  
    - 5000



ExternalName: it helps to connect to teh service running outside of K8s cluster through mapping Service to dns name. IP address not allowed. 

apiVersion: V1
kind: Service
metadata:
 name: patment-gateway
Spec:
  type: ExternalName
  externalName: payment-gateway-api.external-provider.com


Ingress Controller: It is responsible for managing external access to services within the cluster. It allows you to define rules for routing external HTTP and HTTPS traffic to different services based on the host, path, or other criteria.it can be deployed through Helm charts, YAML manifests, or other deployment tools.
Make sure to deplo ingress controller before use ingress resource. 
1. Nginx Ingress
2. Traefik Ingress
3. HAProxy

---Ingress rules----
Ingress exposes HTTP and HTTPS  routes from outside the cluster to services within cluster. Traffic routing is controlled by rules defined in Ingress resource.
Ingress resource cannot do on its own. We need to have Ingress controller  in order for ingress resource to work through ingress class.  

Note: Reverse proxy and Ingress controller basic difference is ingress controller can dynamically reload configurations which is not the case with Reverse proxy. 
 
----Nginx Ingress-----
It acts as a load balancer and handles the routing of incoming requests to different services within the cluster.

----------Traefik ingress--------
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: ingress-name
spec:
  ingressClassName: nginx  # Specify the Ingress controller type here (e.g., nginx, HAProxy, traefik)
  rules:
    - host: domain-name								# host/name based routing	
      http:
        paths:
          - path: /path1							# path based routing
            pathType: Prefix							# exact or prefix
            backend:
              service:
                name: path1-service-name  # Specify the service name
                port:
                  number: 80
          - path: /path2
            pathType: Prefix
            backend:
              service:
                name: path2-service  # Specify the service name
                port:
                  number: 80


----------------------------------K8s Volumes------------------------------
volume is dir accessible to all containers running in pod. containers are ephemeral so to persist the data, K8s provide volumes. 
Ephemeral volumes: 
1. emptyDir(pod): temp storage created and managed by the lifecycle of pod. volume is empty on pod creation and volume gets deleted when pod stopped,rescheduled etc
apiVersion: v1
kind: Pod
metadata:
  name: example-pod
spec:
  containers:
  - name: example-container
    image: nginx:latest
    volumeMounts:
    - name: temp-volume					#same volume can be mount by multiple containers. 
      mountPath: /data
  volumes:
  - name: temp-volume
    emptyDir: {}


2. hostPath(node): The hostPath volume allows a pod to use a file or directory on the host machine as storage. if pod gets rescheduled to other node then directory or file gets created on that particular node from scratch. if same path exist then pod use the same. 
apiVersion: v1
kind: Pod
metadata:
  name: hostpath-pod
spec:
  containers:
  - name: container1
    image: nginx:latest
    volumeMounts:
    - name: hostpath-volume
      mountPath: /data
  volumes:
  - name: hostpath-volume
    hostPath:
      path: /path/on/host
      type: File or Directory or FileorCreate or DirectoryorCreate

----------------K8s Environment variables-----------
Env variables in K8s can be of key-value pairs, ConfigMaps or Secrets. Refer them using env or envFrom keys. 

#plain text
env:
 - name: DB_NAME
   value: "postgres"

kubectl run -it test --env="APP=test" --image=busybox-alpine --rm -- printenv

Note: to inject pod's data as containers env variable
env:
 - name: POD_NAME							--> use ["$(POD_NAME)"] to print env variable in containers in args
   valueFrom:
     frieldRef:
       fieldPath: metadata.name
 - name: POD_IP
   valueFrom:
     fieldRef:
       fieldPath: status.hostIP
         

---configMap--------
provide file as configuration. A configMap used to store tha non-confidential data in key-value pairs. pods can consume  ConfigMaps as 
1. Env variables
2. command line args
3. Configuration file in volumes. 

But if configuration have sensitive info then use secrets. configmap is namespaced obj. unlike env var, if these files(mounted as configmap) changes then new file pushed (eventualy consistent) to runnng pods without need of pod restart

kubectl create configmap (cm) app-config --from-literal=BACKEND_COLOR=blue
kubectl create cm app-config --from-file=./myconfig				--> here myconfig file name will be key and file content wil be value when use file

Note: cm has data field instead of spec in configMap manifest file. when we provide file as configuration then filename wil be key and its content will be value but its of no use so mount file as configMap and use it.
If we use configMap inside pod then configMap must be available otherwise pod will fail due to dependancy.  

apiVersion: v1
kind: ConfigMap
metadata:
  name: db_details
data:
  db_name: mysql
  db_host: mysql.wolrdpay.local
  db_env: prod
---
env:						--> using config map as env variable
  - name: DB_NAME
    valueFrom:
      configMapKeyRef:
        name: db_details
        key: db_name
envFrom:					--> if want to refer all variables from configmap
- configMapRef:
    name: db_detais


---to mount configMap file 
apiVersion: v1
kind: ConfigMap
metadata:
  name: nginx-config
data:
  custom-nginx.conf: |
    # custom-nginx.conf
    server {
        listen 80;
        server_name localhost;

        location / {
            root /usr/share/nginx/html;
            index index.html;
        }
    }
---
apiVersion: v1
kind: Pod
metadata:
  name: nginx-pod
spec:
  containers:
  - name: nginx-container
    image: nginx:latest
    volumeMounts:
    - name: nginx-config-volume
      mountPath: /etc/nginx/conf.d/
  volumes:
  - name: nginx-config-volume
    configMap:
      name: nginx-config
      items:						--> used to rename file to new name
      - key: custom-nginx.conf						
        path: nginx.conf


-----Secrets--------
Secret is K8s obj used to store confidential data like pass,ssh key, tokens etc. same as configMap but specifically intended to hold confidential data.
Pods can consue secrets as
1. Env var
2. configuration file in volume
3. pull secrets by kubelet

Note: secrets are stored unencrypted (base64 encoded) in etcd. anyone with API access update it so use RABC to restrict users. while creating secrets through yaml put base64 values. 

secret types:
1. generic (Opaque)
2. tls
3. docker-registry

kubectl create secret generic my-opaque-secret --from-literal=username=my-username --from-literal=password=my-password
kubectl create secret tls tls-secret --cert=path/to/tls-certificate-file --key=path/to/tls-private-key-file
kubectl create secret docker-registry registry-secret --docker-server=my-registry.io --docker-username=my-username --docker-password=my-password 


apiVersion: v1
kind: Secret
metadata:
  name: db_credentials
data:
  DB_USER: base64 encoded value
  DB_PASS: base64 encoded value
---
env:
  - name: DB_USERNAME
    valueFrom:
      secretKeyRef:
        name: db_credentials
        key: DB_USER
or
envFrom:					--> use all keys from secret
- secretRef:
    name: db_credentials 

--------------------------------------Manual scheduling------------------
manual scheduling is sometimes needed  to ensure that certain pods only scheduled on nodes with specialized hardware like ssd or to co-locate services to communicate frequently (DB-apps pods to reduce latency)
K8s offer several ways to schedule pods
1. nodeName: 
simplest form but due to limitations not used. 
apiVersion: v1
kind: pod
metadata: 
  name: nginx
spec:
 containers:
 - name: nginx
   image: nginx:latest
   ports:
   - containerPort: 80
 nodeName: minikune-04

limitations: if node does not exist or node does not have enough resources or nodes in cloud are unpredictable 
 
2. nodeSelector: 
its same as nodeName however node selector use labels to assign pod on node. limitation is if node does not have label then pod fails.sets hard rules. 

3. Node affinity and anti-affinity:

4. Inter-pod affinity and anti-affinity
5. Taints and toleration

 


Self draining K8s node: 
when performing maintainance, you may sometimes need to remove K8s node from service. To do this, you can drain the node. 
Containrs running on node will be gracefully terminated (and potentially rescheduled to another node)
kubctl drain node-name
kubectl drain node-name --ignore-daemonsets   (when draining a node you may need to ignore daemonsets i.e pods that are tied to each node. If you have any daemonset pods running on node then likely use --ignore-daemonset flag.

uncordon node: 
If node remains part of cluster, You can allow pods to run on node again when maintanance is complete using 
kubectl uncordon node-name
Note: uncordoning the node will not guarantee that pods get restored on same node again. 

backig up and restoring ETCD cluster data:
ETCD is backed data storage solution as such all K8s objects, apps and configurations are stored in etcd. so likely to backup cluster data by backing up etcd.
ETCDCTL_API=3 ectdctl --entpoints $ENDPOINT snapshot save <file-name>
ETCDCTL_API=3 etcdctl snapshot restore <file-name>    --> you need to supply additional parametrs as restore operation creates a new logical cluster.

K8s object management:
you can use the kubectl to deploy applications, inspect and manage cluster resources and view logs. 
kubectl get <object type> objectname -o <output-yaml/json/wide> --sort-by <JSONPath> --selector <label>
kubectl create -f <filename>   --- if we try to create object already exist an error will occur.
kubctl apply -f <filename>     ---same as create but if we apply on already existed object then it will modify.
kubectl delete <object type> ojbect-name
kubectl exec <pod-name> -c <container-name> -- <command>   -->used to run commands inside container.if pod has multiple conainers then specifiy container with -c otherwise optional.

kubectl declarative commands --> define object using the data structure like yaml/json. kubectl apply -f <filename>
kubectl imperative commands --> define objects using kubectl commands. kubectl create deployment my-deployment -- image=nginx --dry-run -o yaml. X create pods using it.
kubectl scale deployment my-deployment replica=5 or --replica 5 --record     ---> record a command

RBAC in K8s:It allows you to control what users are allowed to do and access within your cluster. example: we can use RABC to allow dev to read metadata and logs from
K8s pods but not make any changes to them. 

Cluster----------------------------------------    Roles and ClusterRoles are K8s objects that define a set of permissions. Role define permissons within perticuler
|                                             |    namespace and cluster role defines a cluster wide permissions not specific to single namespace.
|   Namespace---------     ClusterRole------  |
|   |  Role          |    |  permissions   |  |
|   |  permissions   |    |                |  |    RoleBinding and ClusterRoleBinding are objects that connect roles and cluster roles to users.
|   |                |    |                |  |
|   |______ ↑________|    |_______ ↑_______|  |
|                                             |
|     RoleBinding         ClusterRoleBindding |
____________ ↑______________________ ↑________|           
                     ↑
             Users/ServiceAccounts

K8s Service Account: In K8s, Service account is an account used by container processes within pods to authenticate with K8s API.if your pods needs to communicate with 
K8s API then you can use service accounts to control their access. A service account object can be created with some YAML just like other K8s object.
apiVersion: v1
kind: ServiceAccount
metadata: 
 name: my-serviceaccount

you can manage access control for service accounts just like any other user, using RABC objects. 

Inspecting pod resource usage: 
Kubernates metrics server: In order to view metrics about the resources, pods and containers are using, we need an add-on to collect and provide that data. One such 
add-on is kubernates metrics server. Once we installed that add-on we can use top command to view data about resource usage in pods and nodes.  
kubectl top pod --sort-by <JSONpath> --selector <label>


Pods and Containers:
1. managing application configuration: When you are running appls in K8s, you may pass dynamic values to apps at runtime to control how they behave known as Apps Confign.
   ConfigMaps --> you can store configuration data in K8s using cm. configMaps store data in the form of key-value map. cm data can be passed to container apps. 
   Secrets --> similar to configMaps but are designed to store sensitive data such as passwords or API keys. create and used similar to cm. 
   Environment variable --> you can pass cm and secrets as env variables. These variables will be visible to container process at runtime. 
   ConfigurationVolumes --> cm and secret data passed to container in the form of mounted volumes. 

2. managing container resources: 
   Resource requests -->  allows you to define an amt of resources(cpu or memory) you expect a comtainer to use. K8s scheduler will use resource requests to avoid 
   scheduling pods on nodes that do not have enough resources. Note: scheduler wil not spin up pods unless have enough resources beforehand. 
   Resource Limits --> provide a way fr you to limit resources container can use. Container runtime is responsible to enforce resource limits.

3. Building self healing pods with restart policy:  K8s automatically restart containers when they fail or problem arises. 
   Always  --> default restart policy in K8s. containers always be restarted if they stop, or completed successfully
   On-Failure --> Will restart container only if container process exits with error code or containr detect to be unhealthy by livenes probe. X after successfully completed. 
   Never  --> always cause the pods container to never be restarted. even if container exits or liveness probe fails. 

4. Multi-container pods: K8s pods can have >1 containers called multi-container pods. In M-C pods containers share resources such as network(can communicate 
with another container on any port even if port X exposed to cluster) and storage.They can interact with one another, working together to provide functionality. 
Note: Its best practice to keep containers in separate pods unless they to share resources. 
   
5. Init container: are containers that run once during the startup process of pods. Pod can have any number of init containers and they will each run once (in order) to completion. 
you can use init containers to do variety of startup tasks. They are often useful in keeping main containers lighter and more secure by offloading startup tasks to separate container. 


Advanced pod allocation: 
K8s scheduling --> The process of assigning pods to nodes so kubelets can run them. K8s scheduler selects a suitable node for each pod and it takes following into account,
1. resource req vs available node resources
2. Various configuration that affect scheduling using node labels. + you can configure nodeSelector for your pods to limit which node the pod can be scheduled on +
you can also use nodeName to assign specific pod to node directly and bypass scheduling.
kubectl label nodes <nodename> special=true   ---> assigning special=true label to node

DaemonSet --> Automatically runs a copy of pod on each node. DaemonSet will run copy of pod on new node when they are added to the cluster. DaemonSet normally respect 
scheduling rules around nodeLabels, taints and tolerations. If pod would not normally schedule on node, daemonSet will not create copy of pod on that node.

Static pod --> A pod directly managed by kubelet on the node ( created in menifest location on worker node)  but not by K8s API server. They can run even if there is not any K8s API server. 
Kubelets automatically creates static pods from YAML menifest files on worker node. 
Mirror pod ---> Kubelet will create a mirror pod  for each static pod. Mirror pod allow you to see the status of Static pods via K8s API , but you cannot change 
or manage them via API server. 


K8s Deployments: K8s object that defines a desired state for replicaSet(A set of replica pods). Deployment controller seeks to maintain desired state creating,
deleting,  and replacing pods with new configurations. desired state includes,
1. replica: No. of replica pods deployment seek to maintain
2. Selector: used to identify replica pods managed by the deployment.
3. Template: temmplate pod defination used to create replica pods. 

scaling deployment: 
1. edit YAML: 
...
spec
 replica: 5
...
2. kubectl scale
kubectl scale deployment.v1.apps/my-deployment --replicas=5

Rolling update: allows yout o make changes to deployment pods at controlled rate, gradually replacing old pods with new one. This allows you to update your pods without incussing 
downtime. 
kubectl rollout status deployment.v1.apps/my-deployment  --> to see how rolling update is happening
kubectl set image deployment/my-deployment nginx=<version> --record      ----> command for rolling update
Rollback: If an update to the deployent cause a problem , yoyu can roll back the deployment to the previous working state. 


Kubernates network Model: Is set of standards that define how networking between pods behaves. There are variety of implementations of this model inlcuding 
calico network plugin.
Each pod has its unique IP address in cluster. Any pod can reach out other pod using pod's IP address. This creates a Vurtual network that allows Pods to easily 
communicate with each other, regardless of which node they are running. 

DNS in K8s:
K8s virtual network uses DNS to allow pods to locate other pods and services using domain name instead of IP address. 
This DNS runs as a service within cluster. you can easily find it in kube-system namespace. 
pod-domain-names --> all pods in K8s cluster get following domain name form
pod-ip-address.namespace-name.pod.cluster.local         ---> 192.172.17.10.default.pod.cluster.local

Network policies: K8s network policy is a object that allows to control the flow of network communication to and from pods. This allows you to build more secure cluster
network by keeping pods isolated from traffic they do not need.
podSelector: Determines to which pods in namespace network policy applies. podselector selects pods using pod labels. 
Note: By default pods are considered non-isolated and completely open to all communication. If the network policy selects a pod, the pod is considered isolated 
and will only be open to traffic allowed by network policies. 
spec: 
 podSelector: 
  matchLabels: 
    role: db 

network policy can apply to Ingress, Egress or both. from and to selectors --> FromSelector selects ingress traffic that will be allowed
to selector selctes egres traffic that will be allowed. 
spec: 
 ingress: 
  - from: 
    ...
 egress:
  - to: 
    ...
 
from and to selector rules applies to,
1. podselector					2. namespaceSelector		                    3. ipBlock								
select pods to allow traffic to/from             Select namespaces to allow traffic from/to        Select an IP range to allow traffic to/from
spec:                                           spec:                                                spec: 
 ingress:                                         ingress:                                             ingress: 
 - from:                                          - from:                                              - from: 
   - podSelector:                                   - namespaceSelector:                                 - ipBlock:
       matchLabels:                                     matchLabels:                                        cird: 172.0.0.0/16
        app: db                                           app: db
    

ports: Specifies one or more ports that will allow traffic 
spec: 
 ingress: 
  - from: 
    ports: 
     - protocol: tcp
       port: 80


K8s Services: provides a way to expose an application running on set of pods. They provide an abstract way for clients to access applications without needing to be aware 
of application pods. 
                        pods (endpoints --> backend entity to which service routes traffic)
client --> Service--->  pods (endpoints)     
                        pods (endpoints)

Each service has type and Service Type determines how and where the service will expose your application. There are 4 types,
1. ClusterIP  --> expose service only within cluster network
2. NodePort  --> expose service outside the cluster network. (should be between 30000-32767) not suitable for prod use case. 
3. LoadBalancer  --> also expose service outside of cluster network. But they use an external cloud load balancer to do so. works only with cloud platform.
4. Headless service -> used for statefullsets where each pod is not identicle. set  ClusterIp=none to use headless service. so app pod can directly connect using pod ip address.

apiVerison: v1				apiVerison: v1
kind: service				kind: service
metadata:				metadata:
  name: service-type		          name: service-type
spec: 					spec: 
  type: clusterIP			type: nodePort
  selector: 				selector:
     apps: svc-exmaple         		  apps: svc-exmaple     
  ports:                                ports: 
  - protocol: TCP			- protocol: TCP
    port: 80				  port: 80
    targetPort: 80		          targetPort: 80
                                          nodePort: 30080  	

Service DNS names: K8s DNS assigns DNS names to services allowing applications within cluster to locate them easily.service FQDN has following form,
service-name.namespace-name.svc.cluster-domain.example   --> default cluster domain is cluster.local
note: service fqdn can be used to reach service from within any namespace in the cluster. However, pods within same namespace can also simply use servicename.

managing access from outside world to K8s service: 
Ingress is K8s object that manages external access to services in the cluster. An Ingress is capable of providing more functionality than a simple nodePort service, 
such as SSL termination, advanced LoadBalancing, or Name based virtual Hosting. 
external world --> Ingress --> service

Ingress controller: Ingress objects do nothing by themselves. Ingress needs controller for providing extrnal access to your services. ex: nginx,traffic,haproxy etc
Ingress defines a set of routing rules. each rule has set of paths with backend specified. 
ingress controller evaluates rules + manages redirection and entrypoint to cluster
apiVersion: networking.K8s.io/v1
kind: Ingress
metadata: 
  name: my-ingress
spec: 
  rules: 
  - http: 
    paths: 
    - path: /somepath
      pathType: prefix
      backend: 
        service: 
          name: my-service
          port: 
            number: 80
            or
            name: web (for namedPort service)


K8s Storage: 
The container FS is ephemeral. If container is deleted or re-created in K8s then data stored on container FS is lost. 
Volumes: allow you to store data outside of container FS while allowing the container to access data at runtime. Volume offer simple way to 
provide external storage to containers within pod spec. 
syntax: 
Volumes -> In Pod spec, they specify volume type, where and how the data is stored.
volumeMounts -> In Container spec, refer to volumes in pod spec and prvide mount Path

example: 
apiVersion: v1
kind: pod
metadata: 
  name: volume-pod
spec: 
 containers: 
 - name: busybox
   image: busubox
   volumeMounts: 
   - name: my-volume
     mountPath: /output
 - volume: 
   - name: my-volume
     hostPath: 
      path: /data 
      or 
      emptyDir: {}

Note: We can also use mountVolumes to mount same volume on multiple containers. 
Common Volume Types: 
1. hostPath: stores data in specified directory on K8s node.
2. emptyDir: Stores data on dynamically created loc on K8s node. This dir exists as long as pod exist on node. vry useful to share data between containers. 

Persistent volume--> are slightly more advanced form of volume. they allow you to treat storage as abstract resource and consume it 
using pods. 
apiVersion: v1
kind: persistentVolume
metadata: 
  name: my-pv
spec: 
  storageClassName: localdisk
  persistentVolumeReclaimPolicy: recycle        --> determines how the storage rsrc can be reused when pv claims deleted. 
  capacity: 
    storage: 1Gi
  accessModes: 
    - ReadWriteOnce
  hostpath: 
    path: /var/output

pods --> PersistentVolume Claim --> persistent Volume --> External Storage
various volume types support storage method: NFS, cloud storage, configMaps, secrets, simple dir on K8s node etc
persistentVolumeReclaimPolicy types: 
Retain --> keeps all data. admin to manually cleanup data and prepare for storage reusage. 
delete --> delete underlying storage (only works with cloud storage)
recycle --> automatically deletes all data and allowing pv to be reused. 

StorageClass --> administrator can create storagecalss called slow to describe low performance but inexpensive storage and another called fast for high performance
and costly resource. 
apiversion: v1
kind: storageClass
metadata: 
  name: slow or fast
provisioner: kubernates.io/no-provisioner
allowVolumeExpansion: true              --> if not set to true attempting to resize volume throws error. 

PersistentVolumeClaim -> Represents a users req for storage resource. when pvc is created it look for pv. 
apiVersion: v1
kind: PersistentVolumeClaim
metadata: 
 name: my-pvc
spec: 
 storageClassName: localdisk
 accessModes: 
 - ReadWriteOnce
 resources: 
   requests: 
     storage: 100Mi


apiVersion: v1
kind: pod
metadata: 
  name: volume-pod
spec: 
 containers: 
 - name: busybox
   image: busubox
   volumeMounts: 
   - name: my-volume
     mountPath: /output
 - volume: 
   - name: my-volume
     persistentVolumeClaim:
       claimName: my-pvc



--------------------common kubectl commands----------------------
Basic Commands:
kubectl get: Display one or many resources.
kubectl describe: Show details of a specific resource.
kubectl create: Create a resource from a file or from stdin.
kubectl apply: Apply a configuration to a resource.

Viewing Resources:
kubectl get pods: List all Pods in the current namespace.
kubectl get services: List all Services in the current namespace.
kubectl get deployments: List all Deployments in the current namespace.


Interacting with Pods:
kubectl logs <pod-name>: Print the logs from a Pod.
kubectl exec -it <pod-name> -- /bin/bash: Start an interactive shell in a Pod.


Managing Resources:
kubectl delete: Delete resources by filenames, stdin, resources, and names, or by resources and label selector


Managing Configuration:
kubectl config: Modify kubeconfig files.


Troubleshooting:
kubectl describe pod <pod-name>: Show details of a Pod, including events.
kubectl get events: Display events related to Pods and other resources.


Cluster Info:
kubectl cluster-info: Display addresses of the master and services.


Contexts:
kubectl config get-contexts: Display available contexts.


Help:
kubectl --help: Display help for kubectl and its subcommands.
