[KANIKO + BUILDKIT] --> daemonless containerization tools usually used in CI/CD(jenkins) to build container images. 
Docker is an opensource containerization platform for developing, shipping and running application.
Its is designed to solve problem statement of "it works on my machine" i.e code portability.
its client side pgm
act as service
social networking platform

container: Is a standerd unit of software that packages up code, all its dependancies, config, processes, networking, 
os(some chunks) so the application runs quickly and relaibly from one platform to another. 
portable artifcat, easily shared and moved around.

Docker-client
    |
docker-daemon
    | 
local image storage    <------------> DockerHub


Virtual machine 			vs 				Docker

app     | app    |app    |                                        |app       |app    |app    |
config  |config  |config |                                        |config    |config |config |
guest OS|Guest OS|gues OS|                                        |bin/lib   |contnr2|contnr3|
----------------------------                                      ---------------------------------
        Hypervisor                                                  Docker Engine layer (mainly Linux)
----------------------------                                      -----------------------------------
      Operating system 							 Operating system
---------------------------                                       ------------------------------------
        Hardware                                                            Hardware
---------------------------                                       -----------------------------------
1. heavy weight i.e creates standalone                            1. light weight i.e it bundles application dependacies
os on top of os                                                    and use some chunks of kernal to interact with underlying os
2. resource hungry						  2. Not a resource hungry
3. starts and run slow 						  3. Starts and run much fast 
4. cannot run multiple microservices in single vm		  4. can microservice app in docker.  

Running a container:
Busybox - very small, light weight layer of linux
outside world of container does not know whats happening inside container unless explicitely defined. 
container is very very separate thing. 

Syntax: docker run [option] IMAGE:[TAG] [CMD] [ARG]
ex: docker run busybox echo hello world

[OPTION]
-it -> gateway to interact with container. 
-i -> interactive
-t -> tty to give formattable terminal
-d -> detached mode that is to run container at background
--name -> name the container
--restart -> specify when container should restart automatically [values: no, on-failure, always, unless-stopped]
-p <host port>:<container port> -> expose conatiners port by mapping it to host port. (can use multiple times)
--rm -> automatically remove conatiner when it exits. cannot be used with --restart
--memory -> provide hard limit for container
--memory-reservation -> soft limit, used when exceeds hard limit
docker ps --all -> shows all container

ctl + d to exit container
docker start/stop/kill/rm imageid or containername
stop -> allows container to finish its task and stop
kill -> tells container to exit no matter what
rm - before rm container, it must be stopped 

run -> create and start container
exec -> execute command in running container


Upgrading docker engine:
1. install higher version docker
sudo yum install docker-ce=<verson> docker-ce-cli=<version>
downgrading docker:
1. stop docker service
2. uninstall docker-ce and docker-ce-cli
3. install lower docker version 
4. start docker service


Namespaces: linux feature that allows processes to be isolated in terms of resources they see and also used to prevent
diff processes from interacting with one another. docker uses namespaces to isolate conatainers. it allows the containers to work
independently and securely
docker uses following namespaces to isolate rsrsces for containers:
pid,net,ipc,mnt,uts <kernal and version identfier>

Docker images:
Docker image is a immutable file containing the code and components needed to run soft in container. 
Container and images use layered file system. and each layer contains only the diff from previous layer.
image consist of one or more read only layers while container adds one additional writable layer.

         ----------------------------------
         |writable container layer         |
  ------------------------------           |
         |   web app           |           |
  image  |   python            |  conatiner|
         |   base os           |           |
  ------------------------------------------
Layered fs allows multiple images and containers to share same layers results in
1. smaller storage footprints
2. faster image build
3. faster image transfer

docker [management cmd] [sub-cmd]
docker info		--> provides docker info
docker image --help	--> shows all cmmands used with image
docker login -u <username> <url>		--> if you push/pull from private repo

Note: docker pull has max 200 request per 6 hours. 

docker search <imagename>		--> search available images in dockerhub
docker image pull IMAGE:[TAG]        -> download docker images
docker image history IMAGE         -> list the layers used to build image
docker image ls                    -> list the images
docker images inspect IMAGE         --> get detailed info about image
docker image rm IMAGE or docker image rmi IMAGE    -> remove image
docker image rm -f IMAGE   -> remove all tags and delete image. Note -> If we delete image which is being referrenced by running 
container then it will delete tag but not actual image turning into dangling image 
(which has no container + tag refer but consume space on system)

Image cleanup: 
docker system df   --> get info about disk usage on system
docker system df -v --> detailed info
docker sytem prune 	--> delete images/containers not used
docker image prune   -> to delete dangling images (images with no referrnce to tags or containers)
docker image prune -a --> remove all unused images (not used by container)
docker image save <image name> > or -o tarname.tar		--> when you dont have private or public repo and want to share image to other teams. 

docker load < imagename.tar	--> to load image tar

Note: dockerhub credentails are stored under /root/.docker/config.json	--> auth: base64 encoded value for username and password. you can decode using 
echo "encoded value" | base64 -d	--> it will show credentail.
auth will be empty once you are exited from docker. 


------Docker container commands------
docker ps -a 		--> shows all containers
docker container rm <ID>		--> remove container. can't remove running container use -f if want
docker run -d --name=<namee> <image name>	--> create and start container in detached mode with cntainer name
docker log -f <container id>			--> check runtime logs like tail in linux. -f: follow
docker rm -f ${docker ps -aq}			--> remove exited containers forcefully

Note: container goes into exited state when container fails/completes or container asking for something which we are not giving. 
docker run -it ubantu			--> start ubantu and provide interactive mode(STD_IN terminal) with -i and tty terminal with -t (STD_OUT terminal)
ctr+pq		--> to come out of container without exit

docker exec container_ID command		--> execute the command inside running container. 

docker stop: gracefully(grace period is 10sec) shutdown container by sending SIGTERM signal to the main process running inside container. 
docker kill: immediately stops or kills container by sending SIGKILL signal to main process. no grace period like docker stop. 

docker top <container name>		--> get pid of container. you can kill it from host machine too. 
docker container inspect		--> get info about container (IP,port,cmd etc)

doker run -d -p 90:80 --name=web nginx	--> port mapping frm host to container.
docker commit <container_ID> <new name of image>		--> create new image out of running container. 
docker attach container_id		--> run container in foreground mode

Dockerfiles: helps to create our own images. Dockerfile is a set of instructions which are used to construct an image.
(base image,default command,application(.war/ear file),dependancies (runtime)). 
Note: 99% dockerfile starts with FROM, but only ARG can come before FROM in some cases. always use alpine or slim version of base image to resuce image size. 
Google distroless images(find in gcr.io repo)  are speciliazed container images with only apps and its dependancies without any unnecessary extras like pckg manager or shell. docker always looks for Dockerfile to build however if you have diff name then use -f docerfilename

these instructions are called directives.
FROM  -> sets base image + must be first dir in file
LABEL -> defines author name or email address
ARG  -> temp variable used while building an image. 
ENV  ->  set env variables which is used at runtime(container). use -e option to override it while launging container.
WORKDIR  -> set current working dir for subsequet directives ADD, COPY, CMD, ENTRYPOINT etc + can use multiple times. (create + cd)
RUN  -> commands to be executed while building image.[inside image]
run commands has 2 forms 1.shell form(creates shell and excutes cmd)	2. executable form (executes binary directly) 
ex: 
1. RUN echo "hello world"
2. RUN ["/bin/echo","hello world"]
each RUN command adds a layer in image so its bad practice to use multiple RUN to execute muliple commands rather use / to separate shell commands in single run. 
ex: RUN echo "Hello World" && \
        apt-get update && \
        apt-get install utils -y

COPY ->  copy file from host to image
ADD -> similar to COPY but also pull files using URL and extract archive into loose file in images
EXPOSE -> port to be published. port number must be of application. EXPOSE is just for documentation, it will open the port in container. 
VOLUME -> user persistent volume from host machine in containers. used for configuration files not for main files which apps need
CMD  -> sets default commands and/or parameters which can be overridden when the container is run. can have multiple CMD but last one referred. 
ENTRYPOINT --> specifies the main command that will be executed when the container starts. CMD just pass a argument/parameter to an entrypoint. 
ENTRYPOINT + CMD if given both in dockerfile. use --entrypoint=<cmd> to replace entrypoint. only single entrypoint allowed. 

STOPSIGNAL -> specify singal that will use to stop container
HEALTHCHECK --> specify cmd to check container health. health checks are executed periodically by docker itself. if returns 200 then exit status 0 if app crash then exit status 1
ex: HEALTHCHECK CMD curl --fail http://<url> or Heath entrypoint || exit 1

most common: FROM,WORKDIR,COPY,RUN,CMD,ENTRYPOINT

docker context: current dir contents like dockerfile,necessary files sent to docker daemon to build image. The first thing build process does is send the entire context(current woring dir contents) recursively to daemon as tarball. 

----
ex: build python-flask docker image
ARG PYTHON_VERSION:3.8
FROM: python:$PYTHON_VERSION
LABEL author="Shantayya Swami"			--> key=value 
WORKDIR /app
COPY . .
RUN pip install -r requirement.txt
EXPOSE 3000
CMD python ./index.py

docker build -t TAG .   <-t tag and . is dir where dockerfile resides> <--build-arg PYTHON_VERSION=latest if no value provided in dockerfile>
docker run IMAGE

----
ex: build java apps docker image
FROM openjdk:8/11-alpine
WORKDIR /app
COPY /target/demo-0.0.1-SNAPSHOT.jar/ demo.jar
CMD["java","-jar","demo.jar"]

docker build -t demoapp/0.1 . 		--> this will create docker image from dockerfile
docker run demoapp/0.1 hostname		--> here CMD will replace with hostname so app will not execute so better to use ENTRYPOINT for main commands

Note: use can --no-cache if docker dont want to use cache to build image. 
 
-----

Building efficient docker images:
general tips:
1. Put things that are unlikely to change on lower-level layers so to use caching
2. Dont create unnecessary layers
3. Avoid including any unnecessary files, packages in images

you can acheive in 3 ways:
1. Multistage builds
2. alpines base images
3. Distroless base images


Multi-Stage builds  -> Have >1 FROM directive in dockerfile and each FROM directive starts new stage.
each stage begins completely new set of FS layer, allowing you to selectively copy only files that you need from previous layer.
using --from flag to copy files from previous layer.
In multistage builds previous stage creates temp build env and gets deleted once image created.

ex: COPY --from=0 ...
or
FROM <base-image> AS stage1
---
FROM <base-image> AS stage2
COPY --from=stage1 ...
 
Ex: build java apps image 
FROM maven:alpine as BUILD
WORKDIR /app
COPY . .
RUN mvn clean package /app/target/demo.jar

FROM openjdk:8-apline
WORKDIR  /app
COPY --from=BUILD /app/target/demo.jar demo.jar
ENTRYPOINT["java","-jar","demo.jar"]


ex2:
FROM alpine/git as gitclone 
WORKDIR /app
git clone <url>

FROM maven:alpine as build
WORKDIR /app
COPY --from=gitclone /app/target/employee-application.war .
RUN mvn package

FROM tomcat:9.0 as dockerize
COPY --from=build /app/target/*.war /usr/local/tomcat/webapps
EXPOSE 8080

docker build -t shantayyaswami/webapp:v1 . --target=build		--> if we inlcude target build image till specific build stage

-------container security-------------
while building container images we often concern with 2 things
1. The security(through scanning each image layer) --> image layer scanning tools clair,Trivy,Anchore
2. Size of the image(Multistage build or distroless images) 		--> distroless or minimul size images like alpine reduce attack surface. 


---best practices--
1. Do not run container as ROOT
2. avoid copying unnecessary files
3. marge layers
4. using alpine or distroless images as base image
5. using multistage builds
6. health checks
7. avoid exposing unnecessary ports
8. hardcoding credentials

Flattening an Image: sometimes image with few layers perform better and in few cases, you may want to take an image with many 
layers and flatten them into single layer.
1. run a container from image
docker run -d --name flat_container noflat
2. export runnig container(step1) to an archive using docker export
docker export flat_container > flat.tar
3. import an archive as image using docker import
cat flat.tar | docker import - flat:latest


Container Orchestration: all about managing lifecycle of containers(starting,stopping,moving containers across diff hosts,lead balancing,HA etc)
Docker Swarm: feature of docker, which allows you to build distributed cluster of docker machines to run containers.
it facilitates orchestration, high availability and scaling. 

				docker CLI
				   ↓
			 ---------------------------------
 			|	swarm manager 		 |
			| 				 |
	store()	<-------|--scheduler	Discovery service|
			----------------------------------
			API over↓ http		↓
			--------↓------  -------↓---------
			|docer daemon |  | docker daemon |
			|swarm worker |  |swarm worker   |
			|-------------|  |---------------|

manager: manages cluster
worker: run the tasks assigned by scheduler
scheduler: schedule container onto nodes based upon rules and filters
Discovery service: helps swarm manager discover new nodes and fetch their details 
store(key-value db): stores state of cluster. 

docker Swarm		 vs			Kubernates
1. easy installation 				complex installation process
2. manual scaling				supports auto-scaling
3. auto load-balancing				setup manual load-balancer
4. supports third party monitoring tools	Built in monitoring 
5. Integrates docker CLI			need for separate CLI (kubectl)

configuring swarm manager:
1. install docker-ce on swarm manager machine
2. initialize docker swarm with <docker swarm init --advertise-addr private-ip>
3. check <docker info>
4. list current nodes in docker swarm <docker node ls>

configure swarm nodes:
1. install docker-ce on worker node
2. get join command from swarm manager
--> run <docker swarm join-token worker> on manager to get join command
3. run join command on worker nodes provided in step 2
4. verify worker node joined manager or not with <docker node ls> on manager node
 
Backup and restore docker swarm:
1. stop docker service
2. take backup of /var/lib/docker/swarm dir
3. start service


Lock and Unlock swarm cluster: 
docker swarm encrypts sensitive data for security reason (logs on swarm manager + TLS communication btween swarm nodes) 
swarm manages the keys(unenrypted) used for above encryption so Autolock is the feature which locks the swarm.
this gives greater control of keys and can allow greater security. However, it require to unlock swarm every time swarm manager restarts.
<docker swarm unlock>
docker swarm unlock-key   -> if forget key
docker swarm unlock-key --rotate

Enable and disable Autolock:
docker swarm update --autolock=true
docker swarm update --autolock=false

High availability in swarm cluster:
Multiple managers  -> good to have to maintain high availability and fault tolerent swarm. Docker uses Raft consensus algo
to maintain consistent cluster state across multiple managers.
Quorum -> is majority(>half) of managers in swarm. It must be maintained to make changes in swarm cluster state.
Docker recc that you distribute your manager nodes across at least 3 AZ.

Services:
A service is used to run an application on Docker swarm. A service specifies a set of one or more replica tasks. 
These tasks will be auto distributed across nodes in cluster and exec as containers.

Docker service create [OPTIONS] IMAGE ARG
ex:  docker service create --name nginx --replica 3 -p 8080:80
--replica   -> number of replica tasks to create for service.
--name     --> name of the service
-p PUBLISHED_PORT:SERVICE_PORT   --> publish a port so that service can be accessed externally

Templates can be used to give dynamic values to some flags with docker service create
--hostname
--mount
--env
ex: docker service create -env NODE_HOSTNAME="{{.Node.Hostname}}" --replica=3 nginx

docker service ls     -> list current services
docker service ps SERVICE   -> list service tasks
docker service inspect SERVICE   --> get more info about service
docker service update [OPTION] SERVICE   --> make changes to service
docker service rm SERVICE --> delet an existing service

replicated     vs    Global services
replicated services will run req number of replica tasks across swarm cluster.
ex: docker service create --name webserver --replica 3 nginx
global service will run one task on each node.
ex: docker service create --mode global nginx

scaling services: there are 2 ways to scale services
1. using docker service update
ex: docker service update --replica <PELICAS> SERVICE --> docker service update --replica 2 nginx
2. using docker service scale
ex: docker service scale SERVICE=REPLICAS   --> docker service scale nginx=2

Node Labels: 
You can add peices of metadata to your swarm nodes using node labels. Then you can use labels to determine which nodes
tasks will run on. 
Syntax: docker node update --label-add LABEL=VALUE NODE       --> add label to nodes
docker node update --label-add availability_zone=east node1
docker node update --label-add availability_zone=west node2

Node constraints: To run services tasks only on nodes with specific label value use --contraint flag with docker service create
ex: docker service create --contraint node.labels.LABEL=VALUE IMAGE
docker service create --constraint node.labels.availability_zone=east nginx   or
docker service create --constraint node.labels.availability_zone!=east nginx 

Placement-pref: use --placement-pref with the spread strategy to spread tasks evenly across all values of perticular label.
docker service create --placement-pref spread=node.labels.LABEL IMAGE
ex: docker service create --placement-pref spread=node.labels.availability_zone nginx


stateful and stateful applicatons: 
stateful applications hold onto your data across different sessions(like shopping cart thats retains items) while stateless applications does not remember your 
past interactions and treat each new interaction as new one. 

Docker compose: (containers created though docker compose shares the same network)
Tool that allows you to run multi-container application as single service. defined using declarative format in single node(YAML file).
Note:compose does not use swarm mode to deploy services to multiple nodes in swarm. To deploy apps across swarm use
"docker stack deploy"
define docker-compose project:
1. make dir to contain your docker-compose proj
2. change to proj dir
3. add docker-compose.yaml to dir
4. define apps using docker.compose.yaml
ex: 
docker-compose file structure:
version:
service:
  container-01:					--> we can also build images using build keyword instead of using already built images. 
    build: 
      context: <location of dockerfile>
      dockerfile: Dockerfile.prod		--> no need to use dockerfile keyword is name is Dockerfile
    image: <if we use buid then this keyword will tag to image build in above step>  
    env:
    ports:
    volume:
    network:
    restart: 
  container-02:
    command: 					--> its like CMD we mention in dockerfile or running container. 
    image: 
    env:
    port:
    volume:
    network:
volumes: <optinal>				--> whenever keyword appended with 's indicates list
 - my_db
networks: <optional>
 - my_net


Note: volumes and networks are same as 
	docker create volume my_db
	docker create network my_net
docker compose creates custom network for you as default. use depends_on: <list> unser container to use dependency management. 
-----------------------
version: '3'
services: 
   web: 
     image: nginx
     port: 
     - '8080:80'
   redis: 
     image: redis:alpine

version: '3'
services:
  mongodb: 
    image: mongo
    ports: 
     - "27017:27017"
    environment:
     - USERNAME: admin
     - PASS: password
    volumes: 
     mymongo-data: /data/db
  mongo-express:
    image: mongo-express
    ports: 
     - "27017:27017"
    environment:
     - USERNAME: admin  
     - PASS: password
     - mongo-server: mongodb
Volumes: 
  mymongo-data: 
     driver: local 

docker-compose up -f <filename>
docker-compose down -f <filename>

---create and run resources
docker-compose up -d 
docker-compose ps    --> list containers/services running under docker-compose
docker-compose down   --> stop and remove all resources

Docker stack:
A Stack is a collection of interrelated services that can be deployed and scaled as unit. docker stack are similar to the 
multi container apps created using docker-compose. However they can be scaled and executed across swarm cluster.
docker stack deploy -c COMPOSE_FILE STACKNAME    --> Deploy stack to swarm cluster using compose file
docker stack ls			--> list current stacks
dockr stack ps STACKNAME     --> list the tasks associated with stack
docker stack services STACKNAME   --> list services associated with stack
docker stack rm STACKNAME   -> delete stack


Storage and Volumes: (/var/lib/docker/volumes)
image consist of read only layers and when we start container writable layer gets added. writable layer gets deleted when container deleted. To persist changes made in writable layer use Storage and volumes

Storage drivers(Graph drivers):
provides framework for managing the temp, internal storage of containers writable layer. supports variety of storage drivers
1. Overlay2 - File based storage. default for ubantu and CentOs 8+ + efficient for reads
2. devicemapper - block storage + efficient for lot of writes + default for centOS7-
supports 2 modes: 
		1. loop_lvm mode: simulates an additional physical disk using files on local disk + X requied add storage device
+ bad performance + only used for testing.
		2. direct_lvm mode: req an add device + good performance + used for prod
3. aufs - file storage

docker info  ---> shows which storage driver used
Note: Docker automatically select SD based on env + can override with below option
1. set --storage-driver flag while docker start
2. set 'storage-driver' value in /etc/docker/daemon.json 

docker storage consist of layers + you can find location of layered data using ---docker container inspect <conatinrid>---

docker volume: when mounting an external storage to container, use either bind mount or volume
1. Bind mounts:
---mount specific path(with /,$PWD,./) anywhere on host machine to container. 
---not portable 
---depend on host machines FS and dir structure 
2. Named mounts (Volume): 
---storage data on host FS + but storage location is managed by docker.
---More portable 
---can mount same volume to multiple containers.

syntax: docker run --mount [key=value] ...
type = bind, volume,tmpfs					
source/src=volume name or bind mount path
destination/dst/target=path to mount inside container
echo Hello,World -> messege/message.txt
ex: docker run --mount type=bind,src=/home/cloud_user/message,dst=/root,readonly busybox cat /root/message.txt
docker run --mount type=volume,src=my-volume,dst=/root busybox sh -c 'echo Hello > /root/message.txt && cat /root/message.txt'

-v syntax: docker run -v SOURCE:DESTINATION:[OPTIONS]
SOURCE: if this is volume name it will create volume + if this is path then it will create bind mount.
DESTINATION: location to mount the data inside container
OPTIONS: comma separated list of cmnds like ro for readonly 
ex: docker volume create my-volume
docker run -v /home/cloud_user/message:/root:ro busybox cat /root/message.txt		--> create read only mount when multiple containers using it.  
docker run -v my-volume:/root busybox cat /root/mesage.txt				--> my-volume mount point created under docker-home/volume/ dir

mount same volume to multiple container:
docker run --name container1 --mount src=shared-volume ...
docker run --name container2 --mount src=shared-volume ...
or
docker run --name container2 --volumes-from container1 <image>

manage volumes: 
docker volume create VOLUME_NAME
docker volume ls 
docker volume inspect VOLUME_NAME
doker volume rm VOLUME_NAME
docker volume prune				--> volume without data. 

-----setting env variables for container----
docker run --rm -e NAME=DEVOPS nginx:latest env		--> shows env variables from nginx container
docker run --rm --env-file envfile.txt	nginx:latest env		--> if want to set multiple env variables then use env file. 


------------------Docker Networking: (main purpose is to have an isolation)----------------
Bridge network: joins two separate computer networks. works at layer2(ethernet) of OSI model
Docker creates 3 networks when installed: bridge,none and host. when you start docker, default bridge network (docker01) creates automatically and all newly started containers connect to it unless speficied 

Note: docker containers never talk to eac other on IP address rather docker has enbedded DNS(127.0.0.11) which used by containers to connect each other(custom network). not applicable for default networks. 
IP address consist of 4 octets 
ex: 172.17.0.0/16	--> here 172.17 is network subnet and 2^16 are devices. 
  n/w part|host part
containers on the default bridge network can only access other container through IP address. as default bridge network does not have DNS resolver. 

	container1		container2		container3
	eth0			eth1			eth2
	 ↓			 ↓			 ↓
	 ↓______________________ ↓______________________ ↓
        | veth			veth			veth|
	|  ↓^                     ↓^                     ↓  |
	|  -------------------docker01-----------------^    |						--> default bridge network
	|_________________________↓_________________________|
				  ↓ (use NAT to convert container private IP to host public IP)
				Eth0 (Host interface)   --> to connect to outside world
brtcl show				--> shows virtual eth assigned by docker01 to avail containers 

ex: docker network create my_net				--> creates custom bridge network. 
    docker run -d --net my_net nginx:latest --name c1
    docker run -d --net my_net nginx:latest --name c2
    docker exec c1 ping c2		---> will work here as containers are connected to custom network having embedded dns. 

docker network connect bridge c1			--> we are connecting c1 to default network. we can connect to multiple networks. 
docker network disconnect bridge c1

Note:
		n1 (network)			n2
 	---------------			 -------------------
        | c1n1   c2n1 | --> containers	 | c1n2    c2n2    |
        ---------------                  -------------------
Here if c1n1 wants to connect to c1n2 then we should connect 1. n2 to c1n1 or n1 to c1n2  2. create new network n3 and attach c1n1 and c1n2 to n3

Host network: container uses host IP and port number. we cannot have multiple containers running with same port.
pros: network performance (low latency required then) + easy network troubleshoot + legacy system need host configuration 
cons: container ports are exposed to host directly causing sec vul + removes network isolation can be security risk
ex: docker run -d --name c1 --net host nginx: latest

none network: containers will only have local loopback address. cotainers kept in total isolation. when want to disable networking functionality for specific container. 
ex: docker run -d --name c1 --net none ngnix: latest


-----Load balancing in docker------
Process of distributing traffic load to group of backend servers(server pool)

reverse proxy: 
1. hides topological design of backend servers and even in attack takes max hit to protect app servers. 
2. act as cache to reduce load on backend servers.
3. easy to log and audit all requests. 

---load balancing through dns resolution(--net-alias)
docker run -d --net my_net --name c1 shantayyaswami/hello-flask:v1 --net-alias web
docker run -d --net my_net --name c2 shantayyaswami/hello-flask:v1 --net-alias web
docker run -d --net my_net --name c3 shantayyaswami/hello-flask:v1 --net-alias web

doker run -d --net my_net -p 80:80 --name nginx shantayyaswami/nginx --net-alias web

---configure nginx as load balancer:
vi nginx.conf
http--> server--> 
upstream notes<lb name>{			--> add this directory
	server web:5000				--> c1,c2,c3 container application running on port 5000
 }	
server{
 listen 80;
location /{
 # proxy_pass <destination lb name>
   proxy_pass http://notes;
}
}


Docker best practices:
1. Use the docker official images as base image
2. Use Specific base image version (Latest images are tent to be unstable)
3. Use small-sized official images  
4. Optimize the caching image layer (order docker directives to use caching)
5. use .dockerignore to exlude files and folders. 
6. make use of multi-stage builds (for smaller size of images)
7. Use least pivelaged user.   (docker has root permision which can lead to exploitation of app hosted inside container)
8. Scan images for vulnerabilities. 